name: Run SLM and Comment

on:
  workflow_call:
    inputs:
      bot_name:
        type: string
        description: "コメントに表示するBot名"
        default: "SLM Bot"
      model_repo:
        type: string
        description: "HuggingFace repo (e.g. Qwen/Qwen2.5-3B-Instruct-GGUF)"
      model_file:
        type: string
        description: "GGUFファイル名"
      system_prompt:
        type: string
        description: "システムプロンプト"
      issue_number:
        type: number
        description: "issue番号"
      issue_body:
        type: string
        description: "issue本文（caller側で取得済み）"
      max_tokens:
        type: number
        default: 256

jobs:
  run-slm:
    runs-on: ubuntu-latest
    permissions:
      issues: write
    steps:
      - uses: actions/checkout@v6

      - uses: astral-sh/setup-uv@v7
        with:
          enable-cache: true

      - name: Set up venv and install dependencies
        run: |
          uv venv --python 3.12
          uv pip install -r requirements.txt
        env:
          CMAKE_ARGS: "-DGGML_AVX2=ON -DGGML_FMA=ON -DGGML_F16C=ON"

      # actions/cache はステップ実行時にキャッシュを復元し、
      # キャッシュミス時はジョブ終了後の post step で自動的に保存する
      - name: Restore model cache
        id: cache-model
        uses: actions/cache@v5
        with:
          path: ./models
          key: model-${{ inputs.model_repo }}-${{ inputs.model_file }}

      - name: Download model from HuggingFace
        if: steps.cache-model.outputs.cache-hit != 'true'
        run: uv tool run --from huggingface-hub hf download ${{ inputs.model_repo }} ${{ inputs.model_file }} --local-dir ./models

      - name: Validate model file
        run: |
          .venv/bin/python - <<'PY'
          from pathlib import Path

          model_path = Path("./models/${{ inputs.model_file }}")
          if not model_path.is_file():
              raise SystemExit(f"Model file not found: {model_path}")

          with model_path.open("rb") as f:
              magic = f.read(4)
          if magic != b"GGUF":
              raise SystemExit(f"Invalid GGUF header: {model_path}")
          PY

      - name: Run inference
        id: inference
        shell: bash
        run: |
          set -euo pipefail
          DELIMITER=$(openssl rand -hex 16)
          {
            echo "comment<<$DELIMITER"
            echo "$ISSUE_BODY" | .venv/bin/python -m src \
              --model-path "./models/${{ inputs.model_file }}" \
              --system-prompt "$SYSTEM_PROMPT" \
              --max-tokens "${{ inputs.max_tokens }}" \
              --n-threads 4
            echo "$DELIMITER"
          } >> "$GITHUB_OUTPUT"
        env:
          ISSUE_BODY: ${{ inputs.issue_body }}
          SYSTEM_PROMPT: ${{ inputs.system_prompt }}

      - name: Post comment to issue
        shell: bash
        run: |
          {
            echo "**$BOT_NAME** ($MODEL_FILE)"
            echo ""
            echo "$COMMENT_BODY"
          } | gh issue comment ${{ inputs.issue_number }} \
            --repo ${{ github.repository }} \
            --body-file -
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          BOT_NAME: ${{ inputs.bot_name }}
          MODEL_FILE: ${{ inputs.model_file }}
          COMMENT_BODY: ${{ steps.inference.outputs.comment }}
